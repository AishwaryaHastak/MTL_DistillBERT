{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\h4has\\anaconda3\\envs\\senttrans\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a sentence transformer model using any deep learning framework of your choice. \n",
    "This model should be able to encode input sentences into fixed-length embeddings. \n",
    "Test your implementation with a few sample sentences and showcase the obtained embeddings. \n",
    "Describe any choices you had to make regarding the model architecture outside of the transformer backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun set behind the ancient ruins of Machu ...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exploring the bustling markets of Marrakech, t...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After a long hike, adventurers finally reached...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   label\n",
       "0  The sun set behind the ancient ruins of Machu ...  Travel\n",
       "1  Exploring the bustling markets of Marrakech, t...  Travel\n",
       "2  After a long hike, adventurers finally reached...  Travel"
      ]
     },
     "execution_count": 642,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "classification_df = pd.read_csv('data/classification_data.csv')\n",
    "classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun set behind the ancient ruins of Machu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exploring the bustling markets of Marrakech, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After a long hike, adventurers finally reached...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  The sun set behind the ancient ruins of Machu ...      0\n",
       "1  Exploring the bustling markets of Marrakech, t...      0\n",
       "2  After a long hike, adventurers finally reached...      0"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \n",
    "classification_df['label'], unique_labels = pd.factorize(classification_df['label'])\n",
    "num_classes = len(classification_df.label.unique())\n",
    "# label_array = classification_df['label'].to_numpy()\n",
    "# one_hot_labels = torch.eye(num_classes)[label_array]\n",
    "# classification_df['label'] = list(one_hot_labels.numpy())\n",
    "classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encode classification labels\n",
    "# classification_df['label'], unique_labels = pd.factorize(classification_df['label'])\n",
    "# num_classes = len(unique_labels)\n",
    "# print(num_classes)\n",
    "# classification_df['one_hot'] = torch.eye(num_classes)[classification_df['label']]\n",
    "# # print(one_hot)\n",
    "# # classification_df.describe()\n",
    "# classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The tourist's trip to Machu Picchu was ruined ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The chaotic crowds and overwhelming noise made...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After getting lost on the hike, the adventurer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  The tourist's trip to Machu Picchu was ruined ...      0\n",
       "1  The chaotic crowds and overwhelming noise made...      0\n",
       "2  After getting lost on the hike, the adventurer...      0"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sentiment dataset\n",
    "sentiment_df = pd.read_csv('data/sentiment_data.csv')\n",
    "# Transform text label to numerical label \n",
    "sentiment_df['label'], unique_labels = pd.factorize(sentiment_df['label'])\n",
    "num_sentiment = len(unique_labels)\n",
    "sentiment_df.head(3)\n",
    "\n",
    "# one_hot_labels = pd.get_dummies(sentiment_df['label'])\n",
    "# sent_labels = list(one_hot_labels.columns)\n",
    "# print(sent_labels)\n",
    "# # Assign the one-hot encoded array as a new column to the DataFrame\n",
    "# sentiment_df['label'] = list(one_hot_labels.to_numpy(dtype=np.float32))\n",
    "# sentiment_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classification_df['task'] = 'class'\n",
    "sentiment_df['task'] = 'sent'\n",
    "combined_df = pd.concat([classification_df, sentiment_df])\n",
    "\n",
    "# Split the combined dataset\n",
    "train_df, eval_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['task'])\n",
    "\n",
    "# Ensure the columns are correctly formatted\n",
    "train_df = train_df[['sentence', 'label', 'task']]\n",
    "eval_df = eval_df[['sentence', 'label', 'task']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The long-awaited sequel to the blockbuster hit...</td>\n",
       "      <td>3</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After getting lost on the hike, the adventurer...</td>\n",
       "      <td>0</td>\n",
       "      <td>sent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The underdog team's loss in the championship g...</td>\n",
       "      <td>0</td>\n",
       "      <td>sent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  label   task\n",
       "15  The long-awaited sequel to the blockbuster hit...      3  class\n",
       "2   After getting lost on the hike, the adventurer...      0   sent\n",
       "10  The underdog team's loss in the championship g...      0   sent"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(class_labels)\n",
    "# num_sentiment = len(sent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the distilled Bert model to create tokens with a maximum length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sentrans.constants import MODEL_NAME\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = DistilBertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from src.sentrans.constants import TOKEN_MAX_LENGTH\n",
    "\n",
    "# tokenize classification dataset\n",
    "tokenized_train_data = tokenizer(train_df['sentence'].tolist(), padding=True, truncation=True,max_length=TOKEN_MAX_LENGTH, return_tensors='pt')\n",
    "print(tokenized_train_data.keys())\n",
    "\n",
    "# tokenize classification dataset\n",
    "tokenized_eval_data = tokenizer(eval_df['sentence'].tolist(), padding=True, truncation=True,max_length=TOKEN_MAX_LENGTH, return_tensors='pt')\n",
    "print(tokenized_eval_data.keys())\n",
    "\n",
    "# # tokenize sentiment dataset\n",
    "# tokenized_sent_data = tokenizer(sentiment_df['Sentence'].tolist(), padding=True, truncation=True,max_length=TOKEN_MAX_LENGTH, return_tensors='pt')\n",
    "# print(tokenized_sent_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model() function, we will get the embeddings for each token in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "def create_embeddings(tokenized_data):\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([80, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = create_embeddings(tokenized_train_data)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of size 768 are created for each token in each sentence. \n",
    "\n",
    "Hence the size of the embeddings is\n",
    "(number_of_sentences, max_sequence_length, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \n",
      " The sun set behind the ancient ruins of Machu Picchu, casting a golden hue over the Andes mountains. \n",
      " Embedding: \n",
      " tensor([[-0.0950, -0.3981,  0.1139,  ..., -0.2301,  0.4218,  0.1480],\n",
      "        [-0.3557, -0.7085, -0.2099,  ..., -0.0974,  1.0732, -0.3508],\n",
      "        [-0.0573, -0.7418,  0.5210,  ..., -0.2448,  0.3817, -0.0507],\n",
      "        ...,\n",
      "        [-0.0080, -0.4159,  0.4840,  ..., -0.1446,  0.3752, -0.2440],\n",
      "        [-0.1118, -0.3623,  0.2918,  ..., -0.1980,  0.2387, -0.1145],\n",
      "        [-0.2808, -0.3713,  0.3383,  ..., -0.2427,  0.2749, -0.1207]])\n",
      "\n",
      " Embedding Shape: \n",
      " torch.Size([32, 768])\n",
      "\n",
      "Sentence: \n",
      " Exploring the bustling markets of Marrakech, travelers are captivated by the vibrant colors and aromatic spices. \n",
      " Embedding: \n",
      " tensor([[-0.0209,  0.0239, -0.1220,  ...,  0.1055,  0.3616,  0.4078],\n",
      "        [ 0.0467, -0.2368, -0.1285,  ..., -0.1304,  0.2988,  0.1410],\n",
      "        [-0.3015, -0.1668,  0.1919,  ..., -0.1315,  0.0691,  0.0147],\n",
      "        ...,\n",
      "        [ 0.1730,  0.1865,  0.1447,  ...,  0.0804,  0.0303,  0.0735],\n",
      "        [ 0.0425, -0.1519,  0.1248,  ...,  0.0022,  0.0347, -0.0976],\n",
      "        [ 0.0078, -0.0641,  0.1190,  ...,  0.2287, -0.0646, -0.0039]])\n",
      "\n",
      " Embedding Shape: \n",
      " torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "    print('\\nSentence: \\n',classification_df.sentence[i],'\\n Embedding: \\n', embeddings[i])\n",
    "    print('\\n Embedding Shape: \\n', embeddings[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Mutli-Task Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels,task):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.tasks = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        item['task'] = self.tasks[idx]#, dtype=torch.str)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets using the pre-tokenized data\n",
    "train_dataset = NewsDataset(tokenized_train_data, train_df['label'].tolist(),train_df['task'].tolist())\n",
    "eval_dataset = NewsDataset(tokenized_eval_data, eval_df['label'].tolist(), eval_df['task'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  1996,  2146,  1011, 19605,  8297,  2000,  1996, 27858,  2718,\n",
      "        14408, 21967,  9501,  2007,  2049, 14726,  5107,  3896,  1998, 13940,\n",
      "         9994,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(3), 'task': 'class'}\n"
     ]
    }
   ],
   "source": [
    "for d in train_dataset:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader and Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from src.sentrans.constants import BATCH_SIZE\n",
    "batch_size = 4 #BATCH_SIZE\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create an iterator for each DataLoader\n",
    "train_iter = iter(train_loader)\n",
    "eval_iter = iter(eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collator\n",
    "class SentenceDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        # print('batch', batch)\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.tensor([item['label'] for item in batch])#, dtype=torch.long)\n",
    "        tasks = [item['task'] for item in batch]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': labels,\n",
    "            'task': tasks\n",
    "        }\n",
    "\n",
    "data_collator = SentenceDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "from src.sentrans.constants import MODEL_NAME\n",
    "\n",
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, num_classes_topic, num_classes_sentiment):\n",
    "        super(MultiTaskBERT, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier_topic = nn.Linear(self.bert.config.hidden_size, num_classes_topic)\n",
    "        self.classifier_sentiment = nn.Linear(self.bert.config.hidden_size, num_classes_sentiment) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # Take the [CLS] token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        if 'class' in task:\n",
    "            return self.classifier_topic(pooled_output)\n",
    "        elif 'sent' in task:\n",
    "            return self.classifier_sentiment(pooled_output)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert = DistilBertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',             # Output directory\n",
    "    num_train_epochs=5,              # Increase number of epochs for small datasets\n",
    "    per_device_train_batch_size=1,    # Adjust batch size per device during training\n",
    "    per_device_eval_batch_size=1,     # Adjust batch size for evaluation\n",
    "    warmup_steps=10,                  # Reduce warmup steps\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='logs',             # Directory for storing logs\n",
    "    logging_steps=50,                  # Log every 5 steps\n",
    "    eval_strategy=\"steps\",      # Evaluation strategy\n",
    "    # evaluation_strategy=\"no\",      # Evaluation strategy\n",
    "    eval_steps=100,                    # Evaluate every 10 steps\n",
    "    save_steps=200,                    # Save every 20 steps\n",
    "    save_total_limit=1                # Keep only the last saved model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n"
     ]
    }
   ],
   "source": [
    "print(num_classes,num_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom multi-task model\n",
    "model = MultiTaskBERT(num_classes_topic=num_classes, num_classes_sentiment=num_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a custom trainer\n",
    "class SentenceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        labels = inputs['label']\n",
    "        tasks = inputs['task']\n",
    "        # print(input_ids, attention_mask, labels,tasks)\n",
    "        outputs = model(input_ids, attention_mask, tasks) \n",
    "        \n",
    "        # Convert labels to one-hot encoded labels\n",
    "        # num_classes = outputs[0].size(-1)  # Assuming all output logits have the same number of classes\n",
    "        # one_hot_labels = F.one_hot(labels, num_classes=num_classes).float()\n",
    "       \n",
    "        # print(outputs, labels)\n",
    "        return nn.CrossEntropyLoss()(outputs, labels)\n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        tasks = inputs['task']\n",
    "        tasks = inputs['task']\n",
    "        \n",
    "        # We do not need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            logits = []\n",
    "            for i, task in enumerate(tasks):\n",
    "                task_output = model(input_ids=input_ids[i].unsqueeze(0), attention_mask=attention_mask[i].unsqueeze(0), task=task)\n",
    "                logits.append(task_output)\n",
    "\n",
    "        logits = torch.cat(logits)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (None, logits, None)\n",
    "\n",
    "        # labels = torch.argmax(inputs['label'], dim=1)\n",
    "        labels = inputs['label']\n",
    "        return (None, logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Define metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = metric.compute(predictions=preds, references=labels)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = SentenceTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,   # Use the classification dataset for the initial example\n",
    "    eval_dataset=eval_dataset,         # Use the sentiment dataset for evaluation\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [02:26<07:20,  1.47s/it]\n",
      " 12%|█▎        | 50/400 [00:24<03:08,  1.86it/s]\n",
      " 12%|█▎        | 50/400 [00:24<03:08,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2808, 'grad_norm': 26.699920654296875, 'learning_rate': 4.4871794871794874e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 100/400 [00:52<03:03,  1.64it/s]\n",
      " 25%|██▌       | 100/400 [00:52<03:03,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8011, 'grad_norm': 8.837166786193848, 'learning_rate': 3.846153846153846e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      " 25%|██▌       | 100/400 [00:53<03:03,  1.64it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.3234, 'eval_samples_per_second': 15.112, 'eval_steps_per_second': 15.112, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 150/400 [01:24<02:36,  1.60it/s]\n",
      " 38%|███▊      | 150/400 [01:24<02:36,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4922, 'grad_norm': 1.303548812866211, 'learning_rate': 3.205128205128206e-05, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 200/400 [01:52<01:52,  1.77it/s]\n",
      " 50%|█████     | 200/400 [01:52<01:52,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2143, 'grad_norm': 0.1587505340576172, 'learning_rate': 2.564102564102564e-05, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      " 50%|█████     | 200/400 [01:53<01:52,  1.77it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.0985, 'eval_samples_per_second': 18.206, 'eval_steps_per_second': 18.206, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 250/400 [02:25<01:36,  1.55it/s]\n",
      " 62%|██████▎   | 250/400 [02:25<01:36,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1451, 'grad_norm': 0.22588986158370972, 'learning_rate': 1.923076923076923e-05, 'epoch': 3.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 300/400 [02:57<00:59,  1.67it/s]\n",
      " 75%|███████▌  | 300/400 [02:57<00:59,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0465, 'grad_norm': 0.12452692538499832, 'learning_rate': 1.282051282051282e-05, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      " 75%|███████▌  | 300/400 [02:58<00:59,  1.67it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.0158, 'eval_samples_per_second': 19.69, 'eval_steps_per_second': 19.69, 'epoch': 3.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 350/400 [03:31<00:32,  1.55it/s]\n",
      " 88%|████████▊ | 350/400 [03:31<00:32,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0678, 'grad_norm': 0.4132500886917114, 'learning_rate': 6.41025641025641e-06, 'epoch': 4.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:03<00:00,  1.43it/s]\n",
      "100%|██████████| 400/400 [04:03<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0268, 'grad_norm': 0.025203969329595566, 'learning_rate': 0.0, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "                                                 \n",
      "\n",
      "100%|██████████| 400/400 [04:04<00:00,  1.43it/s]\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 1.1839, 'eval_samples_per_second': 16.893, 'eval_steps_per_second': 16.893, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 400/400 [04:08<00:00,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 248.6484, 'train_samples_per_second': 1.609, 'train_steps_per_second': 1.609, 'train_loss': 0.38433348208665846, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=400, training_loss=0.38433348208665846, metrics={'train_runtime': 248.6484, 'train_samples_per_second': 1.609, 'train_steps_per_second': 1.609, 'total_flos': 0.0, 'train_loss': 0.38433348208665846, 'epoch': 5.0})"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 33.25it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = torch.argmax(torch.tensor(predictions), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 0, 0, 1, 2, 4, 2, 3, 4, 3, 0, 4, 2, 2, 1, 0, 0, 0, 1])\n",
      "[0 1 0 0 1 2 4 1 3 4 3 0 4 2 2 1 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_classes)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Training Considerations\n",
    "\n",
    "Implications and advantages of each scenario and explain your rationale as to how the model\n",
    " should be trained given the following:\n",
    "\n",
    "If the entire network should be frozen.\n",
    "If only the transformer backbone should be frozen.\n",
    "If only one of the task-specific heads (either for Task A or Task B) should be frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senttrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
