{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'SENTENCETRANS (Python 3.10.6)' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a sentence transformer model using any deep learning framework of your choice. \n",
    "This model should be able to encode input sentences into fixed-length embeddings. \n",
    "Test your implementation with a few sample sentences and showcase the obtained embeddings. \n",
    "Describe any choices you had to make regarding the model architecture outside of the transformer backbone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun set behind the ancient ruins of Machu ...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exploring the bustling markets of Marrakech, t...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After a long hike, adventurers finally reached...</td>\n",
       "      <td>Travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence   label\n",
       "0  The sun set behind the ancient ruins of Machu ...  Travel\n",
       "1  Exploring the bustling markets of Marrakech, t...  Travel\n",
       "2  After a long hike, adventurers finally reached...  Travel"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "classification_df = pd.read_csv('data/classification_data.csv')\n",
    "classification_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The sun set behind the ancient ruins of Machu ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Exploring the bustling markets of Marrakech, t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After a long hike, adventurers finally reached...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  The sun set behind the ancient ruins of Machu ...      0\n",
       "1  Exploring the bustling markets of Marrakech, t...      0\n",
       "2  After a long hike, adventurers finally reached...      0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode classification labels\n",
    "classification_df['label'], unique_labels = pd.factorize(classification_df['label'])\n",
    "num_classes = len(classification_df.label.unique())\n",
    "\n",
    "# Create a dictionary mapping numerical values to original labels\n",
    "class_label_decode = {index: label for index, label in enumerate(unique_labels)}\n",
    "\n",
    "classification_df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The tourist's trip to Machu Picchu was ruined ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The chaotic crowds and overwhelming noise made...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>After getting lost on the hike, the adventurer...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0  The tourist's trip to Machu Picchu was ruined ...      0\n",
       "1  The chaotic crowds and overwhelming noise made...      0\n",
       "2  After getting lost on the hike, the adventurer...      0"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the sentiment dataset\n",
    "sentiment_df = pd.read_csv('data/sentiment_data.csv')\n",
    "# Transform text label to numerical label \n",
    "sentiment_df['label'], unique_labels = pd.factorize(sentiment_df['label'])\n",
    "num_sentiment = len(unique_labels)\n",
    "\n",
    "# Create a dictionary mapping numerical values to original labels\n",
    "sent_label_decode = {index: label for index, label in enumerate(unique_labels)}\n",
    "\n",
    "sentiment_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Negative', 1: 'Positive', 2: 'Neutral'}\n"
     ]
    }
   ],
   "source": [
    "print(sent_label_decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "classification_df['task'] = 'class'\n",
    "sentiment_df['task'] = 'sent'\n",
    "combined_df = pd.concat([classification_df, sentiment_df])\n",
    "\n",
    "# Split the combined dataset\n",
    "train_df, eval_df = train_test_split(combined_df, test_size=0.2, stratify=combined_df['task'])\n",
    "\n",
    "# Ensure the columns are correctly formatted\n",
    "train_df = train_df[['sentence', 'label', 'task']]\n",
    "eval_df = eval_df[['sentence', 'label', 'task']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>task</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>In response to growing public pressure, the go...</td>\n",
       "      <td>1</td>\n",
       "      <td>sent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>After a long hike, adventurers finally reached...</td>\n",
       "      <td>1</td>\n",
       "      <td>sent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Award-winning actor Smith delivers a powerhous...</td>\n",
       "      <td>3</td>\n",
       "      <td>class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence  label   task\n",
       "47  In response to growing public pressure, the go...      1   sent\n",
       "22  After a long hike, adventurers finally reached...      1   sent\n",
       "16  Award-winning actor Smith delivers a powerhous...      3  class"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes = len(class_labels)\n",
    "# num_sentiment = len(sent_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the distilled Bert model to create tokens with a maximum length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.sentrans.constants import MODEL_NAME\n",
    "\n",
    "# Load the tokenizer and} model\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = DistilBertModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "dict_keys(['input_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "from src.sentrans.constants import TOKEN_MAX_LENGTH\n",
    "\n",
    "# tokenize classification dataset\n",
    "tokenized_train_data = tokenizer(train_df['sentence'].tolist(), padding=True, truncation=True,max_length=TOKEN_MAX_LENGTH, return_tensors='pt')\n",
    "print(tokenized_train_data.keys())\n",
    "\n",
    "# tokenize classification dataset\n",
    "tokenized_eval_data = tokenizer(eval_df['sentence'].tolist(), padding=True, truncation=True,max_length=TOKEN_MAX_LENGTH, return_tensors='pt')\n",
    "print(tokenized_eval_data.keys())\n",
    "\n",
    "# # tokenize sentiment dataset\n",
    "# tokenized_sent_data = tokenizer(sentiment_df['Sentence'].tolist(), padding=True, truncation=True,max_length=TOKEN_MAX_LENGTH, return_tensors='pt')\n",
    "# print(tokenized_sent_data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the model() function, we will get the embeddings for each token in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embeddings\n",
    "def create_embeddings(tokenized_data):\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_mask = tokenized_data['attention_mask']\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([80, 32, 768])\n"
     ]
    }
   ],
   "source": [
    "embeddings = create_embeddings(tokenized_train_data)\n",
    "\n",
    "print(\"Embeddings shape:\", embeddings.shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embeddings of size 768 are created for each token in each sentence. \n",
    "\n",
    "Hence the size of the embeddings is\n",
    "(number_of_sentences, max_sequence_length, embedding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: \n",
      " The sun set behind the ancient ruins of Machu Picchu, casting a golden hue over the Andes mountains. \n",
      " Embedding: \n",
      " tensor([[-0.1222, -0.1128, -0.2531,  ..., -0.1338,  0.0045,  0.4976],\n",
      "        [-0.7761, -0.4002, -0.6663,  ..., -0.2066,  0.2082, -0.0224],\n",
      "        [-0.7013, -0.1719,  0.0192,  ..., -0.3123, -0.3600, -0.1614],\n",
      "        ...,\n",
      "        [-0.2623, -0.1712,  0.2383,  ..., -0.0115, -0.1635,  0.1747],\n",
      "        [-0.0526, -0.1568,  0.2685,  ...,  0.0012, -0.2301,  0.2412],\n",
      "        [-0.0841, -0.1908,  0.1006,  ...,  0.0780, -0.2866,  0.0928]])\n",
      "\n",
      " Embedding Shape: \n",
      " torch.Size([32, 768])\n",
      "\n",
      "Sentence: \n",
      " Exploring the bustling markets of Marrakech, travelers are captivated by the vibrant colors and aromatic spices. \n",
      " Embedding: \n",
      " tensor([[-0.1070, -0.0393,  0.1511,  ...,  0.0250,  0.2682,  0.3162],\n",
      "        [ 0.1359, -0.3061,  0.1702,  ..., -0.1851,  0.1301,  0.1654],\n",
      "        [ 0.0027, -0.2043,  0.2608,  ..., -0.1751, -0.1059,  0.3911],\n",
      "        ...,\n",
      "        [ 0.3709,  0.2008,  0.3203,  ..., -0.2277,  0.1799, -0.0560],\n",
      "        [ 0.8288,  0.1048, -0.3355,  ...,  0.2509, -0.6336, -0.4338],\n",
      "        [-0.3478,  0.3297,  0.5444,  ..., -0.1281, -0.0740, -0.3384]])\n",
      "\n",
      " Embedding Shape: \n",
      " torch.Size([32, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,2):\n",
    "    print('\\nSentence: \\n',classification_df.sentence[i],'\\n Embedding: \\n', embeddings[i])\n",
    "    print('\\n Embedding Shape: \\n', embeddings[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Mutli-Task Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels,task):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        self.tasks = task\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['label'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        item['task'] = self.tasks[idx]#, dtype=torch.str)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets using the pre-tokenized data\n",
    "train_dataset = NewsDataset(tokenized_train_data, train_df['label'].tolist(),train_df['task'].tolist())\n",
    "eval_dataset = NewsDataset(tokenized_eval_data, eval_df['label'].tolist(), eval_df['task'].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  1999,  3433,  2000,  3652,  2270,  3778,  1010,  1996,  2231,\n",
      "         2623, 12720,  8818,  2000,  4769, 22575, 16440,  1012,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'label': tensor(1), 'task': 'sent'}\n"
     ]
    }
   ],
   "source": [
    "for d in train_dataset:\n",
    "    print(d)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom collator\n",
    "class SentenceDataCollator:\n",
    "    def __call__(self, batch):\n",
    "        # print('batch', batch)\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.tensor([item['label'] for item in batch])#, dtype=torch.long)\n",
    "        tasks = [item['task'] for item in batch]\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label': labels,\n",
    "            'task': tasks\n",
    "        }\n",
    "\n",
    "data_collator = SentenceDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "from src.sentrans.constants import MODEL_NAME\n",
    "\n",
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, num_classes_topic, num_classes_sentiment,MODEL_NAME = MODEL_NAME):\n",
    "        super(MultiTaskBERT, self).__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.classifier_topic = nn.Linear(self.bert.config.hidden_size, num_classes_topic)\n",
    "        self.classifier_sentiment = nn.Linear(self.bert.config.hidden_size, num_classes_sentiment) \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0]  # Take the [CLS] token representation\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        if 'class' in task:\n",
    "            return self.classifier_topic(pooled_output)\n",
    "        elif 'sent' in task:\n",
    "            return self.classifier_sentiment(pooled_output)\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',             # Output directory\n",
    "    num_train_epochs=2,              # Increase number of epochs for small datasets\n",
    "    per_device_train_batch_size=1,    # Adjust batch size per device during training\n",
    "    per_device_eval_batch_size=1,     # Adjust batch size for evaluation\n",
    "    warmup_steps=10,                  # Reduce warmup steps\n",
    "    weight_decay=0.01,                # Weight decay\n",
    "    logging_dir='logs',             # Directory for storing logs\n",
    "    logging_steps=50,                  # Log every 5 steps\n",
    "    eval_strategy=\"steps\",      # Evaluation strategy\n",
    "    # evaluation_strategy=\"no\",      # Evaluation strategy\n",
    "    eval_steps=40,                    # Evaluate every 10 steps\n",
    "    save_steps=100,                    # Save every 20 steps\n",
    "    save_total_limit=1                # Keep only the last saved model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 3\n"
     ]
    }
   ],
   "source": [
    "print(num_classes,num_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom multi-task model\n",
    "from src.sentrans.constants import MODEL_NAME\n",
    "model = MultiTaskBERT(num_classes_topic=num_classes, num_classes_sentiment=num_sentiment,MODEL_NAME=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define a custom trainer\n",
    "class SentenceTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        labels = inputs['label']\n",
    "        tasks = inputs['task']\n",
    "        # print(input_ids, attention_mask, labels,tasks)\n",
    "        outputs = model(input_ids, attention_mask, tasks) \n",
    "        return nn.CrossEntropyLoss()(outputs, labels)\n",
    "    \n",
    "    \n",
    "    def prediction_step(self, model, inputs, prediction_loss_only, ignore_keys=None):\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        if inputs['task']:\n",
    "            tasks = inputs['task'] \n",
    "        else: \n",
    "            tasks = ['class','sent'] \n",
    "            with torch.no_grad():\n",
    "                logits = []\n",
    "                for task in tasks:\n",
    "                    task_output = model(input_ids=input_ids[0].unsqueeze(0), attention_mask=attention_mask[0].unsqueeze(0), task=task)\n",
    "                    logits.append(task_output)\n",
    "                print(logits)\n",
    "\n",
    "        # We do not need to calculate gradients for validation\n",
    "        with torch.no_grad():\n",
    "            logits = []\n",
    "            for i, task in enumerate(tasks):\n",
    "                task_output = model(input_ids=input_ids[i].unsqueeze(0), attention_mask=attention_mask[i].unsqueeze(0), task=task)\n",
    "                logits.append(task_output)\n",
    "\n",
    "        logits = torch.cat(logits)\n",
    "\n",
    "        if prediction_loss_only:\n",
    "            return (None, logits, None)\n",
    "        \n",
    "        labels = inputs['label']\n",
    "\n",
    "        # labels = torch.argmax(inputs['label'], dim=1)\n",
    "        return (None, logits, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "# Define metric\n",
    "metric = load_metric(\"accuracy\")\n",
    "# metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    accuracy = metric.compute(predictions=preds, references=labels)\n",
    "    return {\"accuracy\": accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = SentenceTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,   # Use the classification dataset for the initial example\n",
    "    eval_dataset=eval_dataset,         # Use the sentiment dataset for evaluation\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 25%|██▌       | 40/160 [00:20<01:09,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': {'accuracy': 0.8}, 'eval_runtime': 1.0084, 'eval_samples_per_second': 19.833, 'eval_steps_per_second': 19.833, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 50/160 [00:26<01:06,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2256, 'grad_norm': 0.15037643909454346, 'learning_rate': 3.6666666666666666e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 50%|█████     | 80/160 [00:46<00:48,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': {'accuracy': 0.85}, 'eval_runtime': 0.9481, 'eval_samples_per_second': 21.095, 'eval_steps_per_second': 21.095, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 100/160 [00:58<00:36,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1057, 'grad_norm': 0.03196905925869942, 'learning_rate': 2e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 75%|███████▌  | 120/160 [01:14<00:24,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': {'accuracy': 0.85}, 'eval_runtime': 0.9637, 'eval_samples_per_second': 20.753, 'eval_steps_per_second': 20.753, 'epoch': 1.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 150/160 [01:34<00:08,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0409, 'grad_norm': 0.35310038924217224, 'learning_rate': 3.3333333333333333e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 160/160 [01:44<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_accuracy': {'accuracy': 0.85}, 'eval_runtime': 1.2389, 'eval_samples_per_second': 16.143, 'eval_steps_per_second': 16.143, 'epoch': 2.0}\n",
      "{'train_runtime': 104.2467, 'train_samples_per_second': 1.535, 'train_steps_per_second': 1.535, 'train_loss': 0.11689814766868949, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=160, training_loss=0.11689814766868949, metrics={'train_runtime': 104.2467, 'train_samples_per_second': 1.535, 'train_steps_per_second': 1.535, 'total_flos': 0.0, 'train_loss': 0.11689814766868949, 'epoch': 2.0})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  8354,  2083,  1996, 27375,  5380,  1997,  1996,  7139,  1010,\n",
      "        10885,  2545,  2024,  2033,  6491, 11124,  5422,  2011,  1996,  5053,\n",
      "         1997,  1996,  5133,  3470,  1012,   102,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 0, 0, 0]), 'label': tensor(0), 'task': 'class'}\n"
     ]
    }
   ],
   "source": [
    "for i in eval_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 27.26it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, metrics = trainer.predict(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_accuracy': {'accuracy': 0.85},\n",
       " 'test_runtime': 0.8988,\n",
       " 'test_samples_per_second': 22.253,\n",
       " 'test_steps_per_second': 22.253}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions on sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    # Create a DataFrame for the input sentence\n",
    "    pred_df = pd.DataFrame({'sentence': [sentence, sentence], 'label': [0, 0], 'task': ['class', 'sent']})\n",
    "    \n",
    "    # Tokenize the input sentence\n",
    "    inputs = tokenizer(pred_df['sentence'].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=TOKEN_MAX_LENGTH)\n",
    "    \n",
    "    # Create dataset object for the encodings\n",
    "    pred_dataset = NewsDataset(inputs, pred_df.label.tolist(), pred_df.task.tolist())\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions, labels, metrics = trainer.predict(pred_dataset)\n",
    "    \n",
    "    # Get the predicted class and sentiment\n",
    "    predicted_class = torch.argmax(torch.tensor(predictions[0]), dim=-1).item()\n",
    "    predicted_sentiment = torch.argmax(torch.tensor(predictions[1]), dim=-1).item()\n",
    "    \n",
    "    return predicted_class, predicted_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 'The frustration of lost luggage did not overshadow the excitement of exploring the vibrant streets of Barcelona.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 40.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Topic: Travel, Predicted Sentiment: Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topic, sentiment = predict_sentence(sample)\n",
    "print(f\"Predicted Topic: {class_label_decode[topic]}, Predicted Sentiment: {sent_label_decode[sentiment]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"model\")\n",
    "torch.save(model.state_dict(), 'model/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3. Training Considerations\n",
    "\n",
    "Implications and advantages of each scenario and explain your rationale as to how the model\n",
    " should be trained given the following:\n",
    "\n",
    "If the entire network should be frozen.\n",
    "If only the transformer backbone should be frozen.\n",
    "If only one of the task-specific heads (either for Task A or Task B) should be frozen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senttrans",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
